# Real-Time Facial Expression Recognition Toolkit

This repository packages a lightweight, end-to-end pipeline for the FER-2013 dataset: data loading, augmentation, model training, evaluation/analysis, robustness probes, deployment exports, and an OpenCV-powered real-time demo. Everything runs from simple Python entry points so you can reproduce experiments or plug the model into your own projects.

## 1) Environment Setup

1. Use Python 3.10+ and (optionally) create a virtual environment.
2. Install the core dependencies:

```bash
pip install -r requirements.txt
```

The listed requirements cover training, evaluation, and the webcam demo. Notebook-based analyses (`main_experiments.ipynb`, `experiments_noise.ipynb`, `experiments_fairness.ipynb`) additionally use Matplotlib/Seaborn/NumPy/Pandas, which are already imported in the autogenerated `.py` exports.

## 2) Prepare the FER-2013 Dataset

1. Download `fer2013.csv` from Kaggle.
2. Place it at the project root (or note the absolute path). The CSV must contain `emotion`, `pixels`, and `Usage` columns; `Usage` should include `Training`, `PublicTest`, and `PrivateTest` splits.
3. If you want a tiny sample to sanity-check inference, the repository ships `data_example.txt` in the FER-2013 layout.

## 3) Train a Model (CLI)

Launch the bundled trainer, which handles dataloaders, augmentations, checkpoints, and process metadata:

```bash
python -m fer.train \
  --csv path/to/fer2013.csv \
  --epochs 30 \
  --batch-size 128 \
  --in-chans 1 \
  --ckpt-dir runs/exp1
```

Key flags to control experiments:
- `--augmentation {full,baseline}` toggles heavy vs. minimal augmentations for ablations.
- `--mixup` and `--mixup-alpha` enable MixUp during training.
- `--loss {ce,label_smoothing}` with `--label-smoothing-eps` selects the loss function.
- `--width-mult` scales the CNN capacity; `--in-chans 3` replicates grayscale channels for 3-channel backbones.
- `--workers` controls dataloader workers; set to 0 for Windows/CPU-only environments.

Artifacts written under `--ckpt-dir` include `latest.pt`, `best.pt`, a per-epoch `history.csv`, and a `process.json` summary for downstream notebooks and visualizations. The trainer automatically reports best validation accuracy when finished.【F:fer/train.py†L111-L193】【F:fer/train.py†L201-L273】

## 4) Evaluate a Checkpoint

Run standalone evaluation on validation (`PublicTest`) or test (`PrivateTest`) splits:

```bash
python evaluation.py \
  --csv path/to/fer2013.csv \
  --ckpt runs/exp1/best.pt \
  --split test \
  --compute-confusion \
  --save-dir runs/analysis
```

Outputs include JSON metrics, optional confusion-matrix `.npy/.csv` files, and a classification report. The script reconstructs the model with the specified `--in-chans` and `--width-mult` before computing accuracy/loss.【F:evaluation.py†L1-L64】【F:evaluation.py†L66-L108】

## 5) Robustness Sweeps

Probe accuracy under common corruptions (brightness/contrast, blur, JPEG compression, rotations):

```bash
python robust_eval.py \
  --csv path/to/fer2013.csv \
  --ckpt runs/exp1/best.pt \
  --split test \
  --output-dir runs/robustness
```

The script computes clean accuracy, evaluates each corruption/severity, saves a CSV table plus a Matplotlib summary plot, and writes a short markdown summary. Corruption functions come from `fer.robustness` and are wired in `build_corruptions`.【F:robust_eval.py†L1-L113】【F:robust_eval.py†L115-L174】

## 6) Run the Webcam Demo

Use OpenCV to detect faces and stream predictions in real time:

```bash
python - <<'PY'
import torch
from fer import EmotionCNN, FaceDetector, run_realtime_demo

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = EmotionCNN(in_chans=1)
state = torch.load('runs/exp1/best.pt', map_location=device)
model.load_state_dict(state['state_dict'])

detector = FaceDetector(detector_type='haar')  # or 'dnn' if Caffe weights are available
run_realtime_demo(model, detector, device=device, in_chans=1)
PY
```

`run_realtime_demo` handles face detection (Haar/DNN), optional eye-alignment, preprocessing to 48×48, and overlays labels/scores on the video stream. It also prints average FPS plus per-stage timings on exit.【F:fer/inference.py†L1-L124】【F:fer/inference.py†L126-L206】

## 7) Batch Predictions from a FER-2013 CSV

If you have a CSV in FER-2013 format (e.g., `data_example.txt`), generate predictions in bulk:

```bash
python - <<'PY'
import torch
from fer import EmotionCNN, predict_from_fer2013_csv

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = EmotionCNN(in_chans=1)
state = torch.load('runs/exp1/best.pt', map_location=device)
model.load_state_dict(state['state_dict'])

preds = predict_from_fer2013_csv(model, 'data_example.txt', device=device, in_chans=1)
for i, pred in enumerate(preds):
    print(f"Row {i}: {pred['label_name']} (p={pred['confidence']:.3f})")
PY
```

`predict_from_fer2013_csv` builds a dataloader, runs softmax to obtain confidences, and returns label indices/names for each row (optionally filtered by `Usage`).【F:fer/fer2013_io.py†L23-L74】

## 8) Convert Images into FER-2013 Rows

To add your own images to a FER-2013-style CSV (for quick tests or synthetic data generation):

```bash
python - <<'PY'
from fer import image_to_fer2013_row, append_images_to_fer2013_csv

row = image_to_fer2013_row('face.png', emotion=0, usage='PrivateTest')
print(row)

updated = append_images_to_fer2013_csv(['face.png', 'another_face.png'], 'synthetic_fer.csv', emotion=0, usage='PrivateTest')
print(updated.head())
PY
```

The helper loads grayscale images, resizes to 48×48, flattens to a space-delimited pixel string, and either returns a dict or appends rows to a CSV (creating it if missing).【F:fer/fer2013_io.py†L76-L148】

## 9) Export for Deployment

Export TorchScript and ONNX artifacts (optionally quantized) and benchmark latency:

```bash
python export_model.py \
  --checkpoint runs/exp1/best.pt \
  --output-dir exports \
  --in-chans 1 \
  --quantize \
  --evaluate-csv path/to/fer2013.csv
```

The script reconstructs the model, traces TorchScript, exports ONNX with dynamic batch axes, reports file sizes, benchmarks average inference latency, and—if `--quantize`—produces a dynamically quantized TorchScript plus optional accuracy estimates on a subset of the dataset.【F:export_model.py†L1-L126】

## 10) Notebooks & Autogenerated Scripts

- `main_experiments.ipynb` (and its exported `main_experiments.py`) reproduces the core training curves, confusion matrices, robustness summaries, and deployment notes using the `process.json` and `history.csv` emitted by the trainer.【F:main_experiments.py†L1-L113】
- `experiments_noise.ipynb` / `experiments_noise.py` compare cross-entropy vs. label smoothing, generate confusion matrices, and include a helper to inject synthetic label noise for ablations.【F:experiments_noise.py†L1-L119】
- `experiments_fairness.ipynb` similarly inspects group-wise metrics using the synthetic age-like groups built into the dataset wrapper (`FER2013Dataset` uses a deterministic RNG to assign `teen/adult/senior`).【F:fer/data.py†L9-L62】

Launch notebooks with Jupyter or run the exported `.py` files directly after setting `CSV_PATH` and other constants near the top of each script.

## 11) Repository Map

- `fer/` package: datasets, augmentations, models, training loop, inference helpers, robustness utilities, and analysis helpers (confusion matrices/reports).【F:fer/__init__.py†L1-L38】【F:fer/data.py†L1-L62】
- Top-level scripts: `evaluation.py`, `robust_eval.py`, `export_model.py`, `emotion_demo.py`, and notebook exports for experiments and report figures.【F:evaluation.py†L1-L108】【F:robust_eval.py†L1-L174】
- Artifacts: `final_report.tex`, `Project Proposal.pdf`, and related documents summarize the project motivation and findings.

## 12) Tips for Smooth Runs

- Always match `--in-chans` and `--width-mult` between training and downstream scripts (evaluation, export, demo).
- If using GPU, install the appropriate CUDA-enabled PyTorch build from the official instructions before running `pip install -r requirements.txt`.
- For Windows or low-core CPUs, set `--workers 0` to avoid dataloader multiprocessing issues.
- The webcam demo requires a working camera and display; press `q` to exit the stream.
- When experimenting with new CSVs, validate the `pixels` column length (should be 48×48=2304 entries) to avoid runtime errors in `FER2013Dataset`.
