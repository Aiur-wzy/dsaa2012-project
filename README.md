# Real-Time Facial Expression Recognition Toolkit

This repository packages a lightweight, end-to-end pipeline for the FER-2013 dataset: data loading, augmentation, model training, evaluation/analysis, robustness probes, deployment exports, and an OpenCV-powered real-time demo. Everything runs from simple Python entry points so you can reproduce experiments or plug the model into your own projects.

## 1) Environment Setup

1. Use Python 3.10+ and (optionally) create a virtual environment.
2. Install the core dependencies:

```bash
pip install -r requirements.txt
```

The listed requirements cover training, evaluation, and the webcam demo. Notebook-based analyses (`main_experiments.ipynb`, `experiments_noise.ipynb`, `experiments_fairness.ipynb`) additionally use Matplotlib/Seaborn/NumPy/Pandas, which are already imported in the autogenerated `.py` exports.

## 2) Prepare the FER-2013 Dataset

1. Download `fer2013.csv` from Kaggle.
2. Place it at the project root (or note the absolute path). The CSV must contain `emotion`, `pixels`, and `Usage` columns; `Usage` should include `Training`, `PublicTest`, and `PrivateTest` splits.
3. If you want a tiny sample to sanity-check inference, the repository ships `data_example.txt` in the FER-2013 layout.

## 3) Train a Model (CLI)

Launch the bundled trainer, which handles dataloaders, augmentations, checkpoints, and process metadata:

```bash
python -m fer.train --csv path/to/fer2013.csv --epochs 30  --batch-size 128 --in-chans 1  --ckpt-dir runs/exp1
```

Key flags to control experiments:
- `--augmentation {full,baseline}` toggles heavy vs. minimal augmentations for ablations.
- `--mixup` and `--mixup-alpha` enable MixUp during training.
- `--loss {ce,label_smoothing}` with `--label-smoothing-eps` selects the loss function.
- `--width-mult` scales the CNN capacity; `--in-chans 3` replicates grayscale channels for 3-channel backbones.
- `--workers` controls dataloader workers; set to 0 for Windows/CPU-only environments.

Parameter guide:
- `--csv` points to the FER-2013 CSV; relative or absolute paths both work.
- `--epochs` sets total training epochs; increase for more convergence time.
- `--batch-size` controls GPU/CPU memory use and gradient estimate stability.
- `--in-chans` switches between grayscale (1) and 3-channel inputs when using different backbones.
- `--ckpt-dir` is the folder where checkpoints/metrics are written; it will be created if missing.

Artifacts written under `--ckpt-dir` include `latest.pt`, `best.pt`, a per-epoch `history.csv`, and a `process.json` summary for downstream notebooks and visualizations. The trainer automatically reports best validation accuracy when finished.【F:fer/train.py†L111-L193】【F:fer/train.py†L201-L273】

## 4) Evaluate a Checkpoint

Run standalone evaluation on validation (`PublicTest`) or test (`PrivateTest`) splits:

```bash
python evaluation.py   --csv path/to/fer2013.csv   --ckpt runs/exp1/best.pt   --split test   --compute-confusion   --save-dir runs/analysis
```

Outputs include JSON metrics, optional confusion-matrix `.npy/.csv` files, and a classification report. The script reconstructs the model with the specified `--in-chans` and `--width-mult` before computing accuracy/loss.【F:evaluation.py†L1-L64】【F:evaluation.py†L66-L108】

Parameter guide:
- `--csv` location of the FER-2013 data file used to build the evaluation dataloader.
- `--ckpt` checkpoint path to load the trained weights.
- `--split {val,test,all}` chooses `PublicTest`, `PrivateTest`, or the full dataset.
- `--compute-confusion` toggles saving confusion matrix artifacts.
- `--save-dir` output directory for JSON metrics, reports, and optional matrices.

## 5) Robustness Sweeps

Probe accuracy under common corruptions (brightness/contrast, blur, JPEG compression, rotations):

```bash
python robust_eval.py   --csv fer2013.csv   --ckpt runs/exp1/best.pt   --split test   --output-dir runs/robustness
```

The script computes clean accuracy, evaluates each corruption/severity, saves a CSV table plus a Matplotlib summary plot, and writes a short markdown summary. Corruption functions come from `fer.robustness` and are wired in `build_corruptions`.【F:robust_eval.py†L1-L113】【F:robust_eval.py†L115-L174】

Parameter guide:
- `--csv` FER-2013 CSV to read clean and corrupted inputs from.
- `--ckpt` model checkpoint for evaluation.
- `--split` selects the data partition to corrupt (`test` maps to `PrivateTest`).
- `--output-dir` destination for the corruption-wise metrics, plots, and markdown summary.

## 6) Run the Webcam Demo

Start the process by running a Python script (`.py` file) with a simple command line:

```bash
python webcam_demo_cli.py   --ckpt runs/exp1/best.pt   --in-chans 1   --detector haar
```

This wraps `run_realtime_demo` to handle face detection (Haar/DNN), optional eye-alignment, preprocessing to 48×48, and on-screen labels/scores. It also prints average FPS plus per-stage timings on exit.【F:webcam_demo_cli.py†L1-L35】【F:fer/inference.py†L1-L124】【F:fer/inference.py†L126-L206】

Parameter guide:
- `--ckpt` checkpoint containing the trained model weights.
- `--in-chans` sets the expected input channels (1 for grayscale training, 3 for RGB-trained backbones).
- `--detector {haar,dnn}` chooses the face detector; Haar is lightweight, DNN is more robust but heavier.
- Additional optional flags in the script enable eye alignment, resizing hints, and camera index selection if you need a non-default webcam.

## 7) Batch Predictions from a FER-2013 CSV

Start the process by running a Python script (`.py` file) with a command line like:

```bash
python predict_from_csv_cli.py   --ckpt runs/exp1/best.pt   --csv data_example.txt   --usage all   --in-chans 1
```

The script builds a dataloader, runs softmax to obtain confidences, and prints label indices/names for each row (optionally filtered by `Usage`).【F:predict_from_csv_cli.py†L1-L35】【F:fer/fer2013_io.py†L23-L74】

Parameter guide:
- `--ckpt` checkpoint path for the inference model.
- `--csv` FER-2013-style CSV to read inputs from.
- `--usage {Training,PublicTest,PrivateTest,all}` filters which rows to score; `all` disables filtering.
- `--in-chans` must match training input channels; mismatch will degrade results.

## 8) Convert Images into FER-2013 Rows

Start the process by running a Python script (`.py` file) with a command line like:

```bash
python images_to_fer_cli.py   --image face.png   --images face.png another_face.png   --output synthetic_fer.csv   --emotion 0   --usage PrivateTest
```

The helper loads grayscale images, resizes to 48×48, flattens to a space-delimited pixel string, previews a single row, and optionally appends rows to a CSV (creating it if missing).【F:images_to_fer_cli.py†L1-L25】【F:fer/fer2013_io.py†L76-L148】

Parameter guide:
- `--image` single image path to convert; useful when testing one file.
- `--images` list of additional image paths (space-separated) for batch conversion.
- `--output` target CSV; the script creates it with FER-2013 headers if it does not exist and appends rows otherwise.
- `--emotion` integer label (0–6) to assign to every converted row.
- `--usage {Training,PublicTest,PrivateTest}` sets the FER split tag stored alongside the pixels.
- The CLI previews one generated row in the terminal before writing, so you can double-check pixel strings.

## 9) Export for Deployment

Export TorchScript and ONNX artifacts (optionally quantized) and benchmark latency:

```bash
python export_model.py   --checkpoint runs/exp1/best.pt   --output-dir exports   --in-chans 1   --quantize   --evaluate-csv fer2013.csv
```

The script reconstructs the model, traces TorchScript, exports ONNX with dynamic batch axes, reports file sizes, benchmarks average inference latency, and—if `--quantize`—produces a dynamically quantized TorchScript plus optional accuracy estimates on a subset of the dataset.【F:export_model.py†L1-L126】

Parameter guide:
- `--checkpoint` weights to export and benchmark.
- `--output-dir` folder where TorchScript/ONNX/metadata files are saved.
- `--in-chans` ensures the exported model uses the correct input channel count.
- `--quantize` toggles dynamic quantization for smaller TorchScript artifacts.
- `--evaluate-csv` optional FER-2013 CSV path to compute accuracy after export.

## 10) Notebooks & Autogenerated Scripts

- `main_experiments.ipynb` (and its exported `main_experiments.py`) reproduces the core training curves, confusion matrices, robustness summaries, and deployment notes using the `process.json` and `history.csv` emitted by the trainer.【F:main_experiments.py†L1-L113】
- `experiments_noise.ipynb` / `experiments_noise.py` compare cross-entropy vs. label smoothing, generate confusion matrices, and include a helper to inject synthetic label noise for ablations.【F:experiments_noise.py†L1-L119】
- `experiments_fairness.ipynb` similarly inspects group-wise metrics using the synthetic age-like groups built into the dataset wrapper (`FER2013Dataset` uses a deterministic RNG to assign `teen/adult/senior`).【F:fer/data.py†L9-L62】

Launch notebooks with Jupyter or run the exported `.py` files directly after setting `CSV_PATH` and other constants near the top of each script.

## 11) Repository Map

- `fer/` package: datasets, augmentations, models, training loop, inference helpers, robustness utilities, and analysis helpers (confusion matrices/reports).【F:fer/__init__.py†L1-L38】【F:fer/data.py†L1-L62】
- Top-level scripts: `evaluation.py`, `robust_eval.py`, `export_model.py`, `emotion_demo.py`, and notebook exports for experiments and report figures.【F:evaluation.py†L1-L108】【F:robust_eval.py†L1-L174】
- Artifacts: `final_report.tex`, `Project Proposal.pdf`, and related documents summarize the project motivation and findings.

## 12) Tips for Smooth Runs

- Always match `--in-chans` and `--width-mult` between training and downstream scripts (evaluation, export, demo).
- If using GPU, install the appropriate CUDA-enabled PyTorch build from the official instructions before running `pip install -r requirements.txt`.
- For Windows or low-core CPUs, set `--workers 0` to avoid dataloader multiprocessing issues.
- The webcam demo requires a working camera and display; press `q` to exit the stream.
- When experimenting with new CSVs, validate the `pixels` column length (should be 48×48=2304 entries) to avoid runtime errors in `FER2013Dataset`.
