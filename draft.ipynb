{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75548b51",
   "metadata": {},
   "source": [
    "A Minimal Real-Time Facial Expression Recognition System: Lightweight CNN on FER-2013 with OpenCV-based Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a72dfa",
   "metadata": {},
   "source": [
    "1.数据集准备与校验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf61485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "FER_CSV_PATH = \"fer2013.csv\"  # 修改为你的实际路径\n",
    "\n",
    "EMOTION_LABELS = {\n",
    "    0: \"Angry\",\n",
    "    1: \"Disgust\",\n",
    "    2: \"Fear\",\n",
    "    3: \"Happy\",\n",
    "    4: \"Sad\",\n",
    "    5: \"Surprise\",\n",
    "    6: \"Neutral\"\n",
    "}\n",
    "\n",
    "df = pd.read_csv(FER_CSV_PATH)\n",
    "\n",
    "print(df.head())\n",
    "print(\"Total samples:\", len(df))\n",
    "print(\"Usage split:\", df[\"Usage\"].value_counts())\n",
    "\n",
    "def describe_split(name):\n",
    "    sub = df[df[\"Usage\"] == name]\n",
    "    cnt = Counter(sub[\"emotion\"].values.tolist())\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    total = len(sub)\n",
    "    for k in sorted(cnt.keys()):\n",
    "        print(f\"{k} ({EMOTION_LABELS[k]}): {cnt[k]} ({cnt[k]/total:.3%})\")\n",
    "    print(\"Total:\", total)\n",
    "\n",
    "for usage in [\"Training\", \"PublicTest\", \"PrivateTest\"]:\n",
    "    if usage in df[\"Usage\"].unique():\n",
    "        describe_split(usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5e8f0d",
   "metadata": {},
   "source": [
    "2.数据加载与预处理 \n",
    "自定义 Dataset（48×48 灰度读取、归一化、通道适配）\n",
    "FER-2013 的 pixels 是空格分隔的 48*48=2304 个像素值（0–255）\n",
    "我们将其读为 numpy 数组，reshape 为 48×48\n",
    "归一化到 [0,1] 再做标准化（使用 ImageNet 或自定义均值方差）\n",
    "通道：\n",
    "若用自定义 CNN：可直接使用单通道 (1×48×48)\n",
    "若想用预训练模型（ResNet 等）：将灰度复制到 3 通道 (3×48×48)\n",
    "这里提供两种通道模式开关：in_chans = 1 或 3。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c909e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FER2013Dataset(Dataset):\n",
    "    def __init__(self, df, usage_filter=None, transform=None, in_chans=1):\n",
    "        \"\"\"\n",
    "        df: pandas DataFrame, 读取自 fer2013.csv\n",
    "        usage_filter: \"Training\" / \"PublicTest\" / \"PrivateTest\" / None\n",
    "        transform: Albumentations 或 torchvision 风格的变换\n",
    "        in_chans: 1 (灰度) 或 3 (复制到 RGB 形式)\n",
    "        \"\"\"\n",
    "        if usage_filter is not None:\n",
    "            df = df[df[\"Usage\"] == usage_filter].reset_index(drop=True)\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.in_chans = in_chans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        label = int(row[\"emotion\"])\n",
    "        pixels = np.fromstring(row[\"pixels\"], dtype=np.uint8, sep=\" \")\n",
    "        img = pixels.reshape(48, 48)  # H, W  (灰度)\n",
    "\n",
    "        # Albumentations 需要 HWC\n",
    "        img = np.expand_dims(img, axis=-1)  # (48, 48, 1)\n",
    "\n",
    "        if self.in_chans == 3:\n",
    "            img = np.repeat(img, 3, axis=-1)  # (48, 48, 3)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            # Albumentations 风格\n",
    "            transformed = self.transform(image=img)\n",
    "            img = transformed[\"image\"]\n",
    "        else:\n",
    "            # 转为 torch\n",
    "            img = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e6b4b",
   "metadata": {},
   "source": [
    "3.数据增强设计（鲁棒性与噪声）\n",
    "这里使用 Albumentations，特点：\n",
    "\n",
    "几何增强：随机平移、旋转、小尺度仿射、水平翻转\n",
    "光照变化：亮度/对比度抖动、Gamma\n",
    "模糊/压缩：高斯模糊、JPEG 压缩\n",
    "随机遮挡（Cutout 类似）\n",
    "MixUp / CutMix：通常在 batch 级别完成，在训练循环中实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db03a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_transform(in_chans=1):\n",
    "    # 归一化参数：灰度时可使用 mean=0.5, std=0.5；3通道可统一 0.5/0.5\n",
    "    if in_chans == 1:\n",
    "        mean = (0.5,)\n",
    "        std = (0.5,)\n",
    "    else:\n",
    "        mean = (0.5, 0.5, 0.5)\n",
    "        std = (0.5, 0.5, 0.5)\n",
    "\n",
    "    return A.Compose([\n",
    "        # 几何增强\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.1, scale_limit=0.1, rotate_limit=15,\n",
    "            border_mode=0, value=0, p=0.7\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),  # 表情相对对称，可以用\n",
    "\n",
    "        # 光照增强\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2,\n",
    "            contrast_limit=0.2,\n",
    "            p=0.5\n",
    "        ),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "\n",
    "        # 模糊/压缩\n",
    "        A.OneOf([\n",
    "            A.GaussianBlur(blur_limit=3),\n",
    "            A.MotionBlur(blur_limit=3),\n",
    "        ], p=0.3),\n",
    "        A.JpegCompression(quality_lower=60, quality_upper=100, p=0.3),\n",
    "\n",
    "        # 随机遮挡（类似 Cutout，模拟眼镜/口罩）\n",
    "        A.CoarseDropout(\n",
    "            max_holes=2,\n",
    "            max_height=12,\n",
    "            max_width=12,\n",
    "            min_holes=1,\n",
    "            fill_value=0,\n",
    "            p=0.5\n",
    "        ),\n",
    "\n",
    "        # 最后归一化+转 Tensor\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transform(in_chans=1):\n",
    "    if in_chans == 1:\n",
    "        mean = (0.5,)\n",
    "        std = (0.5,)\n",
    "    else:\n",
    "        mean = (0.5, 0.5, 0.5)\n",
    "        std = (0.5, 0.5, 0.5)\n",
    "\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f17a0d3",
   "metadata": {},
   "source": [
    "MixUp / CutMix（缓解标签噪声）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"返回混合后的数据与对应标签系数\"\"\"\n",
    "    if alpha <= 0:\n",
    "        return x, y, torch.ones_like(y, dtype=torch.float32), torch.arange(len(y))\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea35f3b",
   "metadata": {},
   "source": [
    "轻量 CNN 模型设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ea118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class DWConvBlock(nn.Module):\n",
    "    \"\"\"Depthwise Separable Conv 块：DWConv + PWConv\"\"\"\n",
    "    def __init__(self, in_ch, out_ch, stride=1):\n",
    "        super().__init__()\n",
    "        self.dw = nn.Conv2d(in_ch, in_ch, kernel_size=3, stride=stride,\n",
    "                            padding=1, groups=in_ch, bias=False)\n",
    "        self.dw_bn = nn.BatchNorm2d(in_ch)\n",
    "        self.pw = nn.Conv2d(in_ch, out_ch, kernel_size=1, bias=False)\n",
    "        self.pw_bn = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dw(x)\n",
    "        x = self.dw_bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pw(x)\n",
    "        x = self.pw_bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class LightweightFERNet(nn.Module):\n",
    "    def __init__(self, in_chans=1, num_classes=7, width_mult=1.0):\n",
    "        super().__init__()\n",
    "        def c(ch):  # 调整通道数\n",
    "            return int(ch * width_mult)\n",
    "\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, c(32), kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(c(32)),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        # 48x48 -> 24x24\n",
    "        self.block1 = nn.Sequential(\n",
    "            DWConvBlock(c(32), c(64), stride=2),\n",
    "            DWConvBlock(c(64), c(64), stride=1),\n",
    "        )\n",
    "        # 24x24 -> 12x12\n",
    "        self.block2 = nn.Sequential(\n",
    "            DWConvBlock(c(64), c(128), stride=2),\n",
    "            DWConvBlock(c(128), c(128), stride=1),\n",
    "        )\n",
    "        # 12x12 -> 6x6\n",
    "        self.block3 = nn.Sequential(\n",
    "            DWConvBlock(c(128), c(256), stride=2),\n",
    "            DWConvBlock(c(256), c(256), stride=1),\n",
    "        )\n",
    "        # 6x6 -> 3x3\n",
    "        self.block4 = nn.Sequential(\n",
    "            DWConvBlock(c(256), c(512), stride=2),\n",
    "        )\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)  # 全局平均池化 -> (B, C, 1, 1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(c(512), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.gap(x)      # (B, C, 1, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# 参数量估算\n",
    "if __name__ == \"__main__\":\n",
    "    model = LightweightFERNet(in_chans=1, num_classes=7, width_mult=0.75)\n",
    "    x = torch.randn(1, 1, 48, 48)\n",
    "    y = model(x)\n",
    "    print(\"Output shape:\", y.shape)\n",
    "    print(\"Total params:\", sum(p.numel() for p in model.parameters())/1e6, \"M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ef16e8",
   "metadata": {},
   "source": [
    "6.损失函数、类别不均衡与优化设置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b31aaf",
   "metadata": {},
   "source": [
    "6.1Label Smoothing 交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca5c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 可根据类别不均衡指定 class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "classes = np.arange(7)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=classes,\n",
    "    y=df[df[\"Usage\"]==\"Training\"][\"emotion\"].values\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "criterion_ce = nn.CrossEntropyLoss(\n",
    "    weight=class_weights_tensor,  # 若不想加权，可以设为 None\n",
    "    label_smoothing=0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4212f8b2",
   "metadata": {},
   "source": [
    "6.2 Focal Loss（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if isinstance(alpha, (list, np.ndarray)):\n",
    "            self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
    "        else:\n",
    "            self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        ce = F.cross_entropy(logits, target, reduction=\"none\",\n",
    "                             weight=self.alpha.to(logits.device) if self.alpha is not None else None)\n",
    "        pt = torch.exp(-ce)\n",
    "        focal_loss = (1 - pt) ** self.gamma * ce\n",
    "        if self.reduction == \"mean\":\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return focal_loss.sum()\n",
    "        return focal_loss\n",
    "\n",
    "# 使用范例：\n",
    "# criterion = FocalLoss(gamma=2.0, alpha=class_weights_tensor)\n",
    "criterion = criterion_ce  # 先用带 label smoothing + class weight 的 CE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6fb0ee",
   "metadata": {},
   "source": [
    "优化器与学习率调度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659c7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightweightFERNet(in_chans=1, num_classes=7, width_mult=0.75).to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40)  # T_max=epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca51d4",
   "metadata": {},
   "source": [
    "7.Dataloader 与训练细节（AMP / 重采样 / 指标）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ea77b4",
   "metadata": {},
   "source": [
    "7.1 DataLoader 与重采样策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0986fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "in_chans = 1  # 或 3\n",
    "train_transform = get_train_transform(in_chans=in_chans)\n",
    "val_transform = get_val_transform(in_chans=in_chans)\n",
    "\n",
    "train_dataset = FER2013Dataset(df, usage_filter=\"Training\",\n",
    "                               transform=train_transform, in_chans=in_chans)\n",
    "val_dataset = FER2013Dataset(df, usage_filter=\"PublicTest\",\n",
    "                             transform=val_transform, in_chans=in_chans)\n",
    "test_dataset = FER2013Dataset(df, usage_filter=\"PrivateTest\",\n",
    "                              transform=val_transform, in_chans=in_chans)\n",
    "\n",
    "# 可选的重采样\n",
    "train_labels = train_dataset.df[\"emotion\"].values\n",
    "class_sample_count = np.array([len(np.where(train_labels == t)[0]) for t in range(7)])\n",
    "weights = 1.0 / class_sample_count\n",
    "samples_weight = np.array([weights[t] for t in train_labels])\n",
    "samples_weight = torch.from_numpy(samples_weight).double()\n",
    "sampler = WeightedRandomSampler(weights=samples_weight,\n",
    "                                num_samples=len(samples_weight),\n",
    "                                replacement=True)\n",
    "\n",
    "batch_size = 128\n",
    "use_sampler = False  # 若想启用重采样，改为 True\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=sampler if use_sampler else None,\n",
    "    shuffle=not use_sampler,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False,\n",
    "    num_workers=4, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ba7f38",
   "metadata": {},
   "source": [
    "训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c856d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "epochs = 40\n",
    "mixup_alpha = 0.2  # 若不想 MixUp，可设为 0\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Train Epoch {epoch}\")\n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if mixup_alpha > 0:\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(imgs, labels, alpha=mixup_alpha)\n",
    "        else:\n",
    "            inputs, targets_a, targets_b, lam = imgs, labels, labels, 1.0\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            outputs = model(inputs)\n",
    "            if mixup_alpha > 0:\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        # 统计预测（注意：使用原标签 labels）\n",
    "        preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    macro_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "\n",
    "    return avg_loss, acc, macro_f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device, desc=\"Val\"):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for imgs, labels in tqdm(loader, desc=desc):\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1).detach().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    macro_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "    cm = confusion_matrix(all_targets, all_preds, labels=list(range(7)))\n",
    "\n",
    "    return avg_loss, acc, macro_f1, cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a39374f",
   "metadata": {},
   "source": [
    "训练主循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb2d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "patience = 7\n",
    "no_improve_epochs = 0\n",
    "best_model_path = \"best_fer_model.pth\"\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(\n",
    "        model, train_loader, optimizer, criterion_ce, device, epoch\n",
    "    )\n",
    "    val_loss, val_acc, val_f1, val_cm = evaluate(\n",
    "        model, val_loader, criterion_ce, device, desc=\"Val\"\n",
    "    )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1(macro): {train_f1:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1(macro): {val_f1:.4f}\")\n",
    "    print(\"Val Confusion Matrix:\\n\", val_cm)\n",
    "\n",
    "    # 早停策略 + 保存最好模型\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        no_improve_epochs = 0\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(\">> Saved new best model.\")\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\">> Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "print(\"Best Val Acc:\", best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f53ab1",
   "metadata": {},
   "source": [
    "最终测试集评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载最佳模型\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "\n",
    "test_loss, test_acc, test_f1, test_cm = evaluate(\n",
    "    model, test_loader, criterion_ce, device, desc=\"Test\"\n",
    ")\n",
    "print(\"\\n=== Test Results ===\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, F1(macro): {test_f1:.4f}\")\n",
    "print(\"Test Confusion Matrix:\\n\", test_cm)\n",
    "\n",
    "# 逐类 F1\n",
    "y_true_all = []\n",
    "y_pred_all = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            outputs = model(imgs)\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "        y_pred_all.extend(preds)\n",
    "        y_true_all.extend(labels.cpu().numpy())\n",
    "\n",
    "per_class_f1 = f1_score(y_true_all, y_pred_all, average=None, labels=list(range(7)))\n",
    "for i, f1v in enumerate(per_class_f1):\n",
    "    print(f\"Class {i} ({EMOTION_LABELS[i]}): F1 = {f1v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f37f1a",
   "metadata": {},
   "source": [
    "模型压缩与部署优化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee9ed51",
   "metadata": {},
   "source": [
    "知识蒸馏（ResNet-18 → 轻量 CNN 学生）\n",
    "思路：\n",
    "\n",
    "教师：预训练或在 FER 上微调的 ResNet-18（3 通道）\n",
    "学生：你已有的 LightweightFERNet（1 或 3 通道）\n",
    "总损失 = 真实标签交叉熵 + 蒸馏 KL 散度\n",
    "超参：温度 T、蒸馏权重 alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ad9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "num_classes = 7\n",
    "\n",
    "teacher = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "teacher.fc = nn.Linear(teacher.fc.in_features, num_classes)\n",
    "teacher.load_state_dict(torch.load(\"resnet18_fer.pth\"))  # 你预先训练好的权重\n",
    "teacher.eval().to(device)\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3aa78b",
   "metadata": {},
   "source": [
    "蒸馏损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae411392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, targets, T=4.0, alpha=0.5, ce_weight=1.0):\n",
    "    \"\"\"\n",
    "    student_logits, teacher_logits: (B, num_classes)\n",
    "    targets: ground truth labels\n",
    "    \"\"\"\n",
    "    # 硬标签交叉熵\n",
    "    ce = F.cross_entropy(student_logits, targets) * ce_weight\n",
    "\n",
    "    # 软标签 KL 散度\n",
    "    # 注意：KLDivLoss 默认 input 是 log-prob，target 是 prob\n",
    "    log_p_s = F.log_softmax(student_logits / T, dim=1)\n",
    "    p_t = F.softmax(teacher_logits / T, dim=1)\n",
    "    kd = F.kl_div(log_p_s, p_t, reduction=\"batchmean\") * (T * T)\n",
    "\n",
    "    return alpha * kd + (1 - alpha) * ce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862cfb51",
   "metadata": {},
   "source": [
    "蒸馏训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ead4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = LightweightFERNet(in_chans=3, num_classes=num_classes, width_mult=0.75).to(device)\n",
    "optimizer = torch.optim.AdamW(student.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40)\n",
    "\n",
    "T = 4.0\n",
    "alpha = 0.7\n",
    "\n",
    "def train_one_epoch_kd(student, teacher, loader, optimizer, device, epoch):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"KD Train Epoch {epoch}\")\n",
    "    for imgs, labels in pbar:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher(imgs)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            student_logits = student(imgs)\n",
    "            loss = distillation_loss(student_logits, teacher_logits, labels, T=T, alpha=alpha)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = student_logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    macro_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
    "    return avg_loss, acc, macro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e07f9",
   "metadata": {},
   "source": [
    " 剪枝与稀疏化（结构化通道剪枝）\n",
    "思路：\n",
    "\n",
    "在训练/蒸馏阶段，对卷积层添加 L1 正则促使通道稀疏\n",
    "使用类似 torch.nn.utils.prune.ln_structured 对卷积层按通道剪枝\n",
    "剪枝后进行短期微调恢复精度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5890032",
   "metadata": {},
   "source": [
    " 引入 L1 稀疏正则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b3f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_channel_sparsity(model, lambda_l1=1e-5):\n",
    "    l1_loss = 0.0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            l1_loss += m.weight.abs().sum()\n",
    "    return lambda_l1 * l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad896ae7",
   "metadata": {},
   "source": [
    "训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "    student_logits = student(imgs)\n",
    "    loss_main = distillation_loss(student_logits, teacher_logits, labels, T=T, alpha=alpha)\n",
    "    loss = loss_main + l1_channel_sparsity(student, lambda_l1=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10fa019",
   "metadata": {},
   "source": [
    "通道剪枝\n",
    "建议：先用较小剪枝率（0.2–0.3）试水，剪后再用小学习率（如 1e-4）微调 10–20 个 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c8e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "def structured_prune_model(model, amount=0.3):\n",
    "    \"\"\"\n",
    "    对所有 Conv2d 按输出通道 L1-norm 剪枝 amount 比例\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            prune.ln_structured(\n",
    "                module,\n",
    "                name=\"weight\",\n",
    "                amount=amount,\n",
    "                n=1,         # L1\n",
    "                dim=0        # 沿输出通道剪\n",
    "            )\n",
    "            prune.remove(module, \"weight\")  # 使剪枝永久化\n",
    "    return model\n",
    "\n",
    "# 使用示例：\n",
    "student = structured_prune_model(student, amount=0.3)  # 剪 30% 输出通道"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d14b56a",
   "metadata": {},
   "source": [
    " 量化（PTQ / QAT）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675d9b9f",
   "metadata": {},
   "source": [
    "3.1 推理后量化（Post-Training Quantization, PTQ, INT8）\n",
    "PyTorch 动态量化（对 Linear, LSTM 等）很方便，CNN 的真正 INT8 部署通常要依赖后端（如 ONNX Runtime / TensorRT）。给一个基础的动态量化示例（对最后 FC 层）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2220546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    student.cpu(),\n",
    "    {nn.Linear},  # 只对 Linear 做动态量化\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "torch.save(quantized_model.state_dict(), \"student_quant_dynamic.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3c22b",
   "metadata": {},
   "source": [
    "3.2 量化感知训练（QAT，简略示例）\n",
    "PyTorch QAT 流程简要："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d22094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization import (\n",
    "    get_default_qat_qconfig,\n",
    "    prepare_qat,\n",
    "    convert\n",
    ")\n",
    "\n",
    "student_qat = LightweightFERNet(in_chans=1, num_classes=7, width_mult=0.75)\n",
    "student_qat.train()\n",
    "\n",
    "student_qat.qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "student_qat = prepare_qat(student_qat)   # 插入 FakeQuantize 模块\n",
    "\n",
    "# 用你原来的训练循环再训练 5–10 个 epoch（lr 可小一些）\n",
    "# ...\n",
    "\n",
    "student_qat.eval()\n",
    "student_int8 = convert(student_qat)\n",
    "torch.save(student_int8.state_dict(), \"student_qat_int8.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa674327",
   "metadata": {},
   "source": [
    "4. 导出 ONNX / OpenVINO / TFLite\n",
    "4.1 导出 ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec93c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.randn(1, 1, 48, 48, device=device)\n",
    "student.eval().to(device)\n",
    "torch.onnx.export(\n",
    "    student,\n",
    "    dummy,\n",
    "    \"fer_student.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}, \"logits\": {0: \"batch\"}},\n",
    "    opset_version=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#在 PC 上可用 ONNX Runtime\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "ort_session = ort.InferenceSession(\"fer_student.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def onnx_infer(batch_np):  # batch_np: (B,1,48,48) float32\n",
    "    ort_inputs = {\"input\": batch_np}\n",
    "    logits = ort_session.run([\"logits\"], ort_inputs)[0]\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee1653",
   "metadata": {},
   "source": [
    "OpenVINO（x86/Intel）\n",
    "mo --input_model fer_student.onnx --input_shape [1,1,48,48] --data_type FP16 --output_dir openvino_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659df0ca",
   "metadata": {},
   "source": [
    "4.3 TFLite（移动端）\n",
    "在 Python 用 torch.onnx 导出后，用 onnx-tf 转成 TF，然后使用 TFLite Converter；或\n",
    "直接重构一个 TF/Keras 版本的网络，在 Keras 里训练/加载 PyTorch 权重，然后："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1048adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "4.3 TFLite（移动端）\n",
    "在 Python 用 torch.onnx 导出后，用 onnx-tf 转成 TF，然后使用 TFLite Converter；或\n",
    "直接重构一个 TF/Keras 版本的网络，在 Keras 里训练/加载 PyTorch 权重，然后："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"saved_model_dir\")\n",
    "# PTQ 示例：float16 或 INT8\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "open(\"fer_student.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1a4af",
   "metadata": {},
   "source": [
    "4.4 精度-延迟-体积记录\n",
    "对于每种模型（Teacher, Student, Pruned, INT8）记录：\n",
    "\n",
    "模型大小：os.path.getsize(\"model.xxx\") / 1024**2\n",
    "测试集准确率 / macro F1\n",
    "平均推理时间（多次前向取均值）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a567e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os\n",
    "\n",
    "def benchmark_model(model, dataloader, device, n_warmup=10, n_runs=100):\n",
    "    model.eval().to(device)\n",
    "    # 预热\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            _ = model(x)\n",
    "            if i >= n_warmup:\n",
    "                break\n",
    "    # 正式计时\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            t0 = time.time()\n",
    "            _ = model(x)\n",
    "            t1 = time.time()\n",
    "            times.append((t1 - t0) / x.size(0))\n",
    "            if i >= n_runs:\n",
    "                break\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "model_size_mb = os.path.getsize(\"student.pth\") / (1024**2)\n",
    "latency = benchmark_model(student, test_loader, device)\n",
    "print(f\"Size: {model_size_mb:.2f} MB,  Avg Latency: {latency*1000:.2f} ms / image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2718e",
   "metadata": {},
   "source": [
    "人脸检测与对齐（OpenCV 链路）\n",
    "1. 人脸检测器选择与对比\n",
    "两种常用方案：\n",
    "\n",
    "Haar/LBP 级联（经典，CPU 轻量，但对小脸和侧脸较弱）\n",
    "DNN Res10 SSD (deploy.prototxt + res10_300x300_ssd_iter_140000.caffemodel)，OpenCV cv2.dnn，精度和鲁棒性更好，但略慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e795df",
   "metadata": {},
   "source": [
    "1.1 Haar 检测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "haar_face = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "def detect_faces_haar(gray_frame):\n",
    "    faces = haar_face.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n",
    "    # 返回 [x,y,w,h] 列表\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17908ae2",
   "metadata": {},
   "source": [
    "1.2 DNN Res10 SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5155d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_proto = \"deploy.prototxt\"\n",
    "dnn_model = \"res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(dnn_proto, dnn_model)\n",
    "\n",
    "def detect_faces_dnn(frame, conf_threshold=0.5):\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "                                 (300, 300), (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    boxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            box = detections[0, 0, i, 3:7] * [w, h, w, h]\n",
    "            (x1, y1, x2, y2) = box.astype(\"int\")\n",
    "            boxes.append((x1, y1, x2-x1, y2-y1))\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6f27b",
   "metadata": {},
   "source": [
    "2. 人脸对齐\n",
    "常见做法：\n",
    "\n",
    "使用 5 点或 68 点关键点（如 dlib / retinaface / opencv face landmarks）\n",
    "将双眼中心连线旋转到水平，平移/缩放对齐到标准模板\n",
    "这里给一个简化版本（假设你已有 68 点或 5 点关键点 landmarks）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_face(img, landmarks, output_size=(48, 48)):\n",
    "    # landmarks: shape (5,2) 或 (68,2)，这里假设 5 点：左眼、右眼、鼻尖、嘴左、嘴右\n",
    "    desired_left_eye = (0.35, 0.35)\n",
    "    desired_face_width, desired_face_height = output_size\n",
    "\n",
    "    left_eye, right_eye = landmarks[0], landmarks[1]\n",
    "    # 计算眼睛中心与角度\n",
    "    eye_center = ((left_eye[0]+right_eye[0]) / 2.0,\n",
    "                  (left_eye[1]+right_eye[1]) / 2.0)\n",
    "    dy = right_eye[1] - left_eye[1]\n",
    "    dx = right_eye[0] - left_eye[0]\n",
    "    angle = np.degrees(np.arctan2(dy, dx))  # 逆时针角度\n",
    "\n",
    "    # 期望眼距\n",
    "    dist = np.sqrt((dx ** 2) + (dy ** 2))\n",
    "    desired_dist = (1.0 - 2*desired_left_eye[0]) * desired_face_width\n",
    "    scale = desired_dist / dist\n",
    "\n",
    "    # 获取仿射变换矩阵\n",
    "    eyes_center = eye_center\n",
    "    M = cv2.getRotationMatrix2D(eyes_center, angle, scale)\n",
    "\n",
    "    # 调整平移\n",
    "    tX = desired_face_width * 0.5\n",
    "    tY = desired_face_height * desired_left_eye[1]\n",
    "    M[0, 2] += (tX - eyes_center[0])\n",
    "    M[1, 2] += (tY - eyes_center[1])\n",
    "\n",
    "    aligned = cv2.warpAffine(img, M, (desired_face_width, desired_face_height),\n",
    "                             flags=cv2.INTER_CUBIC)\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d85bec",
   "metadata": {},
   "source": [
    "如果你暂时没有关键点模型，可以先直接依赖检测框："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c158f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_face(frame, box, margin=0.2, out_size=48):\n",
    "    x, y, w, h = box\n",
    "    cx, cy = x + w//2, y + h//2\n",
    "    side = int(max(w, h) * (1 + margin))\n",
    "    x1 = max(cx - side//2, 0)\n",
    "    y1 = max(cy - side//2, 0)\n",
    "    x2 = min(cx + side//2, frame.shape[1])\n",
    "    y2 = min(cy + side//2, frame.shape[0])\n",
    "    face = frame[y1:y2, x1:x2]\n",
    "    face = cv2.resize(face, (out_size, out_size))\n",
    "    return face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f2f49",
   "metadata": {},
   "source": [
    "3. 批量与缓存优化、跟踪减少检测频率\n",
    "对每 N 帧做一次完整检测，其余帧使用跟踪器（KCF/CSRT）跟踪人脸框\n",
    "对每个检测到的脸维护一个 cv2.TrackerKCF_create() 或 TrackerCSRT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc17860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tracker():\n",
    "    return cv2.legacy.TrackerKCF_create()  # 或 TrackerCSRT_create()\n",
    "\n",
    "trackers = []  # 列表[(tracker, id), ...]\n",
    "\n",
    "def update_trackers(frame):\n",
    "    new_boxes = []\n",
    "    for tracker, tid in trackers:\n",
    "        ok, box = tracker.update(frame)\n",
    "        if ok:\n",
    "            new_boxes.append((tid, box))  # box: (x,y,w,h)\n",
    "    return new_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142eae70",
   "metadata": {},
   "source": [
    "实时推理原型（摄像头管线）\n",
    "总体结构：\n",
    "\n",
    "VideoCapture 读取摄像头\n",
    "多线程：一个线程采集帧，一个线程做推理（可选）\n",
    "使用上面的人脸检测 + 对齐\n",
    "将对齐后的 48×48 灰度输入轻量 CNN / 量化模型\n",
    "在画面上叠加边框、表情标签、置信度\n",
    "键盘交互：切换检测器 / 对齐开关等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e92d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 假设你已经有: student (PyTorch 模型), val_transform 或推理预处理\n",
    "EMOTION_LABELS = [\"Angry\",\"Disgust\",\"Fear\",\"Happy\",\"Sad\",\"Surprise\",\"Neutral\"]\n",
    "\n",
    "use_dnn_detector = True  # 按键切换\n",
    "use_alignment = False    # 是否启用对齐（若有关键点模块）\n",
    "frame_skip_for_detect = 10  # 每10帧做一次检测\n",
    "frame_count = 0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 简单预处理函数（与训练时一致）\n",
    "def preprocess_face(face_bgr):\n",
    "    gray = cv2.cvtColor(face_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(gray, (48, 48))\n",
    "    # [0,1] & 标准化 (0.5, 0.5)\n",
    "    face = face.astype(np.float32) / 255.0\n",
    "    face = (face - 0.5) / 0.5\n",
    "    face = face[None, None, :, :]  # (1,1,48,48)\n",
    "    return torch.from_numpy(face).to(device)\n",
    "\n",
    "student.eval().to(device)\n",
    "\n",
    "prev_time = time.time()\n",
    "fps_display = 0.0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_count += 1\n",
    "    orig = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 1) 检测／跟踪\n",
    "    if frame_count % frame_skip_for_detect == 1:\n",
    "        if use_dnn_detector:\n",
    "            face_boxes = detect_faces_dnn(frame)\n",
    "        else:\n",
    "            face_boxes = detect_faces_haar(gray)\n",
    "        # TODO: 初始化 trackers 等，这里简化为每次直接用检测结果\n",
    "    # 否则你可以用 tracker.update() 来更新 face_boxes\n",
    "\n",
    "    # 2) 对每个检测到的人脸做推理\n",
    "    if len(face_boxes) > 0:\n",
    "        batch_faces = []\n",
    "        crops = []\n",
    "        for box in face_boxes:\n",
    "            face_crop = crop_face(frame, box, margin=0.2, out_size=48)\n",
    "            crops.append((box, face_crop))\n",
    "            tensor = preprocess_face(face_crop)\n",
    "            batch_faces.append(tensor)\n",
    "\n",
    "        batch = torch.cat(batch_faces, dim=0)  # (N,1,48,48)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                logits = student(batch)\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "        # 3) 在图像上绘制结果\n",
    "        for (box, face_crop), prob in zip(crops, probs):\n",
    "            x, y, w, h = box\n",
    "            label_id = prob.argmax()\n",
    "            label = EMOTION_LABELS[label_id]\n",
    "            conf = prob[label_id]\n",
    "\n",
    "            cv2.rectangle(orig, (x, y), (x+w, y+h), (0,255,0), 2)\n",
    "            text = f\"{label}: {conf:.2f}\"\n",
    "            cv2.putText(orig, text, (x, y-10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "\n",
    "    # 4) 帧率统计\n",
    "    now = time.time()\n",
    "    dt = now - prev_time\n",
    "    prev_time = now\n",
    "    fps = 1.0 / dt\n",
    "    fps_display = 0.9*fps_display + 0.1*fps  # 平滑\n",
    "    cv2.putText(orig, f\"FPS: {fps_display:.1f}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,255), 2)\n",
    "\n",
    "    cv2.imshow(\"FER Demo\", orig)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:  # ESC退出\n",
    "        break\n",
    "    elif key == ord('d'):  # 切换检测器\n",
    "        use_dnn_detector = not use_dnn_detector\n",
    "    elif key == ord('a'):  # 切换对齐\n",
    "        use_alignment = not use_alignment\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a10e8",
   "metadata": {},
   "source": [
    "鲁棒性与公平性评测\n",
    "1. 受控鲁棒性测试\n",
    "对验证/测试集施加可控扰动，然后在每种扰动条件下评测准确率、F1 与混淆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb843af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_brightness_contrast(img, alpha=1.0, beta=0):  # alpha:对比度, beta:亮度\n",
    "    new = cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "    return new\n",
    "\n",
    "def add_gaussian_blur(img, ksize=3, sigma=1.0):\n",
    "    return cv2.GaussianBlur(img, (ksize, ksize), sigmaX=sigma)\n",
    "\n",
    "def jpeg_compress(img, quality=50):\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
    "    _, enc = cv2.imencode('.jpg', img, encode_param)\n",
    "    dec = cv2.imdecode(enc, 1)\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76248594",
   "metadata": {},
   "source": [
    "1.2 姿态扰动（yaw/pitch/roll）\n",
    "在真实视频中通过头部转动收集样本较现实；若只基于 2D 图像，可通过仿射变换模拟一定 yaw/roll："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae49f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotate(img, max_angle=30):\n",
    "    h, w = img.shape[:2]\n",
    "    angle = np.random.uniform(-max_angle, max_angle)\n",
    "    M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1.0)\n",
    "    return cv2.warpAffine(img, M, (w, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5af92",
   "metadata": {},
   "source": [
    "1.3 时间稳定性（预测抖动）\n",
    "在实时视频上对同一人进行连续预测\n",
    "记录每一帧的预测标签，计算随时间的变化次数（抖动）\n",
    "可计算 top-2 准确率：若真实标签在 top2 置信度中记为正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99a1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_accuracy(logits, labels, k=2):\n",
    "    topk = torch.topk(logits, k, dim=1).indices.cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    correct = sum([1 if labels[i] in topk[i] else 0 for i in range(len(labels))])\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420eca0",
   "metadata": {},
   "source": [
    "2. 公平性与偏置评估（代理方式）\n",
    "在缺乏真实人口统计标签的情况下，可以用一些代理特征：\n",
    "\n",
    "肤色估计：简单 RGB/HSV 分析或用预训练的人脸属性模型（如 FairFace）提取\n",
    "性别呈现、年龄段：基于公开人脸属性模型的推理结果作为“proxy label”\n",
    "将测试集分成若干子集（如浅色皮肤组/深色皮肤组、年轻/中年/老年）\n",
    "在每个子集上分别计算：\n",
    "\n",
    "TPR（真实阳性率）、FNR\n",
    "准确率、macro F1\n",
    "各组之间的差值 ΔTPR, ΔFNR, ΔAcc\n",
    "示意代码（假设你给每张图打了 group_id）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50114a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def group_metrics(y_true, y_pred, groups):\n",
    "    metrics = {}\n",
    "    for g in set(groups):\n",
    "        idx = [i for i, gg in enumerate(groups) if gg == g]\n",
    "        if not idx:\n",
    "            continue\n",
    "        yt = [y_true[i] for i in idx]\n",
    "        yp = [y_pred[i] for i in idx]\n",
    "        acc = accuracy_score(yt, yp)\n",
    "        f1 = f1_score(yt, yp, average=\"macro\")\n",
    "        metrics[g] = {\"acc\": acc, \"f1\": f1}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a701e14",
   "metadata": {},
   "source": [
    "报告时需要明确说明：\n",
    "\n",
    "使用的是代理变量而非真实种族/性别；\n",
    "评测结果只反映模型在这些代理下的差异，存在不确定性；\n",
    "如有明显差异，应在改进数据多样性、重新训练或加权损失后再次评估。\n",
    "3. 误用与安全边界声明（建议文案要点）\n",
    "在项目文档或 UI 中，应明确写出：\n",
    "\n",
    "该模型只适用于娱乐、非安全关键场景；\n",
    "模型对表情的识别不是对“情绪状态”的可靠判断，不能用于：\n",
    "心理健康诊断\n",
    "谈判、招聘、执法等重要决策\n",
    "模型性能在以下情况下会明显下降：\n",
    "极端光照、强遮挡（口罩、墨镜）、大幅姿态偏转\n",
    "模糊或压缩严重的视频\n",
    "声明潜在偏置风险：不同人群在训练数据中的覆盖度不同，可能导致性能差异；\n",
    "推荐在生产环境中始终有人类监督，不应将其作为唯一决策依据。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
